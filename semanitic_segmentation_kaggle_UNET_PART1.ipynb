{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_hjdY6ubfhH",
        "outputId": "ee37176d-6800-461a-92ad-a61393114687"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x   #TensorFlow is a Python library for fast numerical\n",
        "                          #computing created and released by Google."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fojsTeiS2igp",
        "outputId": "951e835a-2cd4-4074-8210-0fbcf66a8b79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2V93ypUZdfIw",
        "outputId": "30c18741-09dd-41ce-bd42-c46eaa0c9892"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.8/dist-packages (2.9.0)\n",
            "Tensorflow ver. 2.9.2\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "from imutils import paths  #A series of convenience functions to make basic\n",
        "                           #image processing functions such as translation, rotation, resizing,\n",
        "                           #skeletonization, displaying Matplotlib images*/\n",
        "\n",
        "from tqdm import tqdm      #used for creating Progress Meters or Progress Bars\n",
        "import numpy as np\n",
        "from glob import glob      #used to return all file paths that match a specific pattern\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import os\n",
        "import tensorflow as tf    #converts regular python code to a callable Tensorflow graph function\n",
        "import datetime\n",
        "import IPython.display as display\n",
        "from IPython.display import clear_output\n",
        "from google.colab import files\n",
        "import math\n",
        "import time\n",
        "tf.__version__\n",
        "!sudo pip3 install keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import *\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')   #it ignores warning messages\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE   #to find a good allocation of its CPU budget across all parameters\n",
        "print(f'Tensorflow ver. {tf.__version__}')  #prints version of our tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuL86zA3jnuv"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "dataset_path = '/content/drive/MyDrive/segmentation_kaggle_dataset_ceci_project_1/new_idd_dataset_kaggle/idd20k_lite/'\n",
        "img_train = dataset_path + 'leftImg8bit/train/'\n",
        "seg_train = dataset_path + 'gtFine/train/'\n",
        "\n",
        "img_val = dataset_path + 'leftImg8bit/val/'\n",
        "seg_val = dataset_path + 'gtFine/val/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAkrqp79Lzh1"
      },
      "source": [
        "# Data Visualization\n",
        "# Sample Training Image\n",
        "# Plotting The Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCEvX0NTmyzT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "outputId": "b8a08505-e0dd-4a11-93b1-d417a1108c69"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-38430a31623c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_train\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"0/024541_image.jpg\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2643\u001b[0m         \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deprecated_parameter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2644\u001b[0m         resample=None, url=None, *, data=None, **kwargs):\n\u001b[0;32m-> 2645\u001b[0;31m     __ret = gca().imshow(\n\u001b[0m\u001b[1;32m   2646\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maspect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m         \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5624\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5626\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5627\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    691\u001b[0m         if (self._A.dtype != np.uint8 and\n\u001b[1;32m    692\u001b[0m                 not np.can_cast(self._A.dtype, float, \"same_kind\")):\n\u001b[0;32m--> 693\u001b[0;31m             raise TypeError(\"Image data of dtype {} cannot be converted to \"\n\u001b[0m\u001b[1;32m    694\u001b[0m                             \"float\".format(self._A.dtype))\n\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Image data of dtype object cannot be converted to float"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAMbElEQVR4nO3bcYikd33H8ffHXFNpGrWYFeTuNJFeGq+2kHRJU4SaYlouKdz9YZE7CG1KyKE1UlAKKZZU4l9WakG41l6pRAWNp3+UBU8CtZGAeDEbEmPuQmQ9bXNRmjOm/iMaQ7/9YybtZL+7mSd3szO39f2ChXme+e3Md4fhfc8881yqCkma9IpFDyDpwmMYJDWGQVJjGCQ1hkFSYxgkNVPDkOQTSZ5O8tgm9yfJx5KsJXk0yTWzH1PSPA05Yrgb2PcS998I7Bn/HAb+4fzHkrRIU8NQVfcDP3yJJQeAT9XICeA1SV4/qwElzd+OGTzGTuDJie0z433fX78wyWFGRxVccsklv3XVVVfN4Oklbeahhx76QVUtvdzfm0UYBquqo8BRgOXl5VpdXZ3n00s/d5L8+7n83iy+lXgK2D2xvWu8T9I2NYswrAB/PP524jrgR1XVPkZI2j6mfpRI8lngeuCyJGeAvwZ+AaCqPg4cB24C1oAfA3+6VcNKmo+pYaiqQ1PuL+A9M5tI0sJ55aOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoMg6RmUBiS7EvyRJK1JHdscP8bktyX5OEkjya5afajSpqXqWFIchFwBLgR2AscSrJ33bK/Ao5V1dXAQeDvZz2opPkZcsRwLbBWVaer6jngHuDAujUFvGp8+9XA92Y3oqR5GxKGncCTE9tnxvsmfRC4OckZ4Djw3o0eKMnhJKtJVs+ePXsO40qah1mdfDwE3F1Vu4CbgE8naY9dVUerarmqlpeWlmb01JJmbUgYngJ2T2zvGu+bdCtwDKCqvga8ErhsFgNKmr8hYXgQ2JPkiiQXMzq5uLJuzX8AbwdI8mZGYfCzgrRNTQ1DVT0P3A7cCzzO6NuHk0nuSrJ/vOz9wG1JvgF8Frilqmqrhpa0tXYMWVRVxxmdVJzcd+fE7VPAW2c7mqRF8cpHSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUDApDkn1JnkiyluSOTda8M8mpJCeTfGa2Y0qapx3TFiS5CDgC/D5wBngwyUpVnZpYswf4S+CtVfVsktdt1cCStt6QI4ZrgbWqOl1VzwH3AAfWrbkNOFJVzwJU1dOzHVPSPA0Jw07gyYntM+N9k64Erkzy1SQnkuzb6IGSHE6ymmT17Nmz5zaxpC03q5OPO4A9wPXAIeCfkrxm/aKqOlpVy1W1vLS0NKOnljRrQ8LwFLB7YnvXeN+kM8BKVf2sqr4DfItRKCRtQ0PC8CCwJ8kVSS4GDgIr69b8C6OjBZJcxuijxekZzilpjqaGoaqeB24H7gUeB45V1ckkdyXZP152L/BMklPAfcBfVNUzWzW0pK2VqlrIEy8vL9fq6upCnlv6eZHkoapafrm/55WPkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySmkFhSLIvyRNJ1pLc8RLr3pGkkizPbkRJ8zY1DEkuAo4ANwJ7gUNJ9m6w7lLgz4EHZj2kpPkacsRwLbBWVaer6jngHuDABus+BHwY+MkM55O0AEPCsBN4cmL7zHjf/0pyDbC7qr74Ug+U5HCS1SSrZ8+efdnDSpqP8z75mOQVwEeB909bW1VHq2q5qpaXlpbO96klbZEhYXgK2D2xvWu87wWXAm8BvpLku8B1wIonIKXta0gYHgT2JLkiycXAQWDlhTur6kdVdVlVXV5VlwMngP1VtbolE0vaclPDUFXPA7cD9wKPA8eq6mSSu5Ls3+oBJc3fjiGLquo4cHzdvjs3WXv9+Y8laZG88lFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWDwpBkX5InkqwluWOD+9+X5FSSR5N8OckbZz+qpHmZGoYkFwFHgBuBvcChJHvXLXsYWK6q3wS+APzNrAeVND9DjhiuBdaq6nRVPQfcAxyYXFBV91XVj8ebJ4Bdsx1T0jwNCcNO4MmJ7TPjfZu5FfjSRnckOZxkNcnq2bNnh08paa5mevIxyc3AMvCRje6vqqNVtVxVy0tLS7N8akkztGPAmqeA3RPbu8b7XiTJDcAHgLdV1U9nM56kRRhyxPAgsCfJFUkuBg4CK5MLklwN/COwv6qenv2YkuZpahiq6nngduBe4HHgWFWdTHJXkv3jZR8Bfhn4fJJHkqxs8nCStoEhHyWoquPA8XX77py4fcOM55K0QF75KKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqRkUhiT7kjyRZC3JHRvc/4tJPje+/4Ekl896UEnzMzUMSS4CjgA3AnuBQ0n2rlt2K/BsVf0q8HfAh2c9qKT5GXLEcC2wVlWnq+o54B7gwLo1B4BPjm9/AXh7ksxuTEnztGPAmp3AkxPbZ4Df3mxNVT2f5EfAa4EfTC5Kchg4PN78aZLHzmXoBbmMdX/PBWw7zQrba97tNCvAr53LLw0Jw8xU1VHgKECS1apanufzn4/tNO92mhW217zbaVYYzXsuvzfko8RTwO6J7V3jfRuuSbIDeDXwzLkMJGnxhoThQWBPkiuSXAwcBFbWrVkB/mR8+4+Af6uqmt2YkuZp6keJ8TmD24F7gYuAT1TVySR3AatVtQL8M/DpJGvADxnFY5qj5zH3ImynebfTrLC95t1Os8I5zhv/YZe0nlc+SmoMg6Rmy8OwnS6nHjDr+5KcSvJoki8neeMi5pyY5yXnnVj3jiSVZGFfsw2ZNck7x6/vySSfmfeM62aZ9l54Q5L7kjw8fj/ctIg5x7N8IsnTm10XlJGPjf+WR5NcM/VBq2rLfhidrPw28CbgYuAbwN51a/4M+Pj49kHgc1s503nO+nvAL41vv3tRsw6dd7zuUuB+4ASwfKHOCuwBHgZ+Zbz9ugv5tWV0Uu/d49t7ge8ucN7fBa4BHtvk/puALwEBrgMemPaYW33EsJ0up546a1XdV1U/Hm+eYHRNx6IMeW0BPsTo/678ZJ7DrTNk1tuAI1X1LEBVPT3nGScNmbeAV41vvxr43hzne/EgVfcz+jZwMweAT9XICeA1SV7/Uo+51WHY6HLqnZutqarngRcup563IbNOupVRhRdl6rzjQ8bdVfXFeQ62gSGv7ZXAlUm+muREkn1zm64bMu8HgZuTnAGOA++dz2jn5OW+t+d7SfT/F0luBpaBty16ls0keQXwUeCWBY8y1A5GHyeuZ3Qkdn+S36iq/1roVJs7BNxdVX+b5HcYXcfzlqr670UPNgtbfcSwnS6nHjIrSW4APgDsr6qfzmm2jUyb91LgLcBXknyX0WfLlQWdgBzy2p4BVqrqZ1X1HeBbjEKxCEPmvRU4BlBVXwNeyeg/WF2IBr23X2SLT4rsAE4DV/B/J3F+fd2a9/Dik4/HFnQCZ8isVzM6KbVnETO+3HnXrf8Kizv5OOS13Qd8cnz7MkaHvq+9gOf9EnDL+PabGZ1jyALfD5ez+cnHP+TFJx+/PvXx5jDwTYzq/23gA+N9dzH6FxdGpf08sAZ8HXjTAl/cabP+K/CfwCPjn5VFzTpk3nVrFxaGga9tGH30OQV8Ezh4Ib+2jL6J+Oo4Go8Af7DAWT8LfB/4GaMjr1uBdwHvmnhtj4z/lm8OeR94SbSkxisfJTWGQVJjGCQ1hkFSYxgkNYZBUmMYJDX/AwqkUdV2nfELAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "img = cv2.imread(img_train + \"0/024541_image.jpg\" , 1)\n",
        "plt.imshow(img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mf5mIdWQnoPk"
      },
      "outputs": [],
      "source": [
        "plt.hist(img.ravel(),256,[0,256])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvok8cfJpjMO"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread(seg_train + \"0/024541_inst_label.png\" , 1)\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uKLBBe4qHbS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "0e76be40-aa10-4e3d-bf9c-c97514c28b4e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-9703f453e98c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'ravel'"
          ]
        }
      ],
      "source": [
        "plt.hist(img.ravel(),256,[0,256])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry19J2nCsYU2"
      },
      "source": [
        "We can see that all the pixel intensity values lie in the range 0–6.\n",
        "\n",
        "To get a more clear picture of the values belonging to each class label, let’s count the values belonging to each class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CH7YIBVWsjU5"
      },
      "outputs": [],
      "source": [
        "plt.hist(img.ravel(),7,[0,7])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qdGbwJYtAOq"
      },
      "outputs": [],
      "source": [
        "# Frequency Distribution\n",
        "# This is to find the exact number of pixels corresponind to each class\n",
        "y = np.bincount(img.flatten())   # flatten() is used to produce a produces a lateral view\n",
        "i = np.nonzero(y)[0]\n",
        "np.vstack((i, y[i])).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcHWBXcVuHVf"
      },
      "source": [
        "We can observe that the class label 5 has the highest count of values and class 1 has the lowest count.\n",
        "We can just consider the value 255 as class label 7 as it will be easier for prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3H2z7cLMW87"
      },
      "source": [
        "# Plotting The Annotated image after intensifying the pixels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxt6y0YzCQAx"
      },
      "outputs": [],
      "source": [
        "def visualizeSegmentationImages(imagepath):\n",
        "    img_seg = cv2.imread(imagepath,0)\n",
        "    # We are going to loop through all the pixel values and multiply them by 40 now\n",
        "    for i in range(len(img_seg)):\n",
        "        for j in range(len(img_seg[0])):\n",
        "            if img_seg[i][j] != 0 or img_seg[i][j] != 255:\n",
        "                # max value will be 6*40=240 which is under 255\n",
        "                img_seg[i][j] *= 40\n",
        "    return img_seg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BS8rfHSCUuV"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Getting the processed segmentation image for visualization\n",
        "img_seg = visualizeSegmentationImages(seg_train + \"0/024541_inst_label.png\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-U229lUJCl_2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Plotting the processed segmentation image\n",
        "plt.imshow(img_seg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjvXdNSOC1-8"
      },
      "outputs": [],
      "source": [
        "# Plotting an Histogram to find the frequency of pixels intensity values.\n",
        "plt.hist(img_seg.ravel(),256,[0,256])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZaFMoaIDE7P"
      },
      "source": [
        "Now, we can easily distinguish the objects after intensifying the pixels as there are colors vary on a wider range, so it is easy to visualize the annotated image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HLscF4SEqLr"
      },
      "outputs": [],
      "source": [
        "# Image size that we are going to use\n",
        "(HEIGHT,WIDTH) = (128,256)\n",
        "# Our images are RGB (3 channels)\n",
        "N_CHANNELS = 3\n",
        "# Scene Parsing has 7 classes (0-6) + `not labeled`\n",
        "N_CLASSES = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgE4BaCdExBu"
      },
      "outputs": [],
      "source": [
        "# The glob module finds all the pathnames matching a specified pattern according to\n",
        "# the rules used by the Unix shell, although results are returned in arbitrary order.\n",
        "\n",
        "TRAINSET_SIZE = len(glob(img_train+'*/*_image.jpg'))\n",
        "print(f\"The Training Dataset contains {TRAINSET_SIZE} images.\")\n",
        "\n",
        "VALSET_SIZE = len(glob(img_val+'*/*_image.jpg'))\n",
        "print(f\"The Validation Dataset contains {VALSET_SIZE} images.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8BfTW4zvNkG"
      },
      "outputs": [],
      "source": [
        "def parse_image(img_path):                     #Load an image and its annotation(mask) and returning a dictionary.\n",
        "    image = tf.io.read_file(img_path)          #reading a image\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    # Our one Image path:\n",
        "    # /idd20k_lite/leftImg8bit/train/024541_image.jpg\n",
        "    # Its corresponding annotation path:\n",
        "    # /idd20k_lite/gtFine/train/024541_label.png\n",
        "    mask_path = tf.strings.regex_replace(img_path, \"leftImg8bit\", \"gtFine\")\n",
        "    mask_path = tf.strings.regex_replace(mask_path, \"_image.jpg\", \"_label.png\")\n",
        "    mask = tf.io.read_file(mask_path)       # Reading the annotation file corresponding the image file\n",
        "    mask = tf.image.decode_png(mask, channels=1)\n",
        "    # In scene parsing, \"not labeled\" = 255\n",
        "    # But it will mess with our N_CLASS = 7\n",
        "    # Since 255 means the 255th class\n",
        "    # Which doesn't exist\n",
        "    mask = tf.where(mask==255, np.dtype('uint8').type(7), mask)\n",
        "    # Note that we have to convert the new value (7)\n",
        "    # With the same dtype than the tensor itself\n",
        "    return {'image': image, 'segmentation_mask': mask}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mf6lPJIE-RM"
      },
      "outputs": [],
      "source": [
        "# tf.data.Dataset.list_files returns a dataset of all files matching one or more glob patterns.\n",
        "\n",
        "train_dataset = tf.data.Dataset.list_files(img_train+'*/*_image.jpg', seed=SEED)\n",
        "train_dataset = train_dataset.map(parse_image)\n",
        "\n",
        "val_dataset = tf.data.Dataset.list_files(img_val+'*/*_image.jpg', seed=SEED)\n",
        "val_dataset = val_dataset.map(parse_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2DFkxPcvVM_"
      },
      "outputs": [],
      "source": [
        "def normalize(input_image, input_mask):\n",
        "    \"\"\"\n",
        "    Rescale the pixel values of the images between 0 and 1 compared to [0,255] originally.\n",
        "    \"\"\"\n",
        "    input_image = tf.cast(input_image, tf.float32) / 255.0\n",
        "    return input_image, input_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKryv8gbkXxh"
      },
      "outputs": [],
      "source": [
        "def load_image_train(datapoint):\n",
        "    \"\"\"\n",
        "    Normalize and resize a train image and its annotation.\n",
        "    Apply random transformations to an input dictionary containing a train image and its annotation.\n",
        "    tf.random.uniform(()) refers to weater the given image is annoted image or normal image, if its greater than 0.5 then its annoted image\n",
        "    \"\"\"\n",
        "    input_image = tf.image.resize(datapoint['image'], (128,128))\n",
        "    input_mask = tf.image.resize(datapoint['segmentation_mask'], (128,128))\n",
        "    if tf.random.uniform(()) > 0.5:\n",
        "        input_image = tf.image.flip_left_right(input_image)\n",
        "        input_mask = tf.image.flip_left_right(input_mask)\n",
        "    input_image, input_mask = normalize(input_image, input_mask)\n",
        "    return tf.convert_to_tensor(input_image), tf.convert_to_tensor(input_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YJlRuC-l_lF"
      },
      "outputs": [],
      "source": [
        "def load_image_test(datapoint):\n",
        "    \"\"\"\n",
        "    Normalize and resize a test image and its annotation.\n",
        "    Since this is for the test set, we don't need to apply any data augmentation technique.\n",
        "    \"\"\"\n",
        "    input_image = tf.image.resize(datapoint['image'], (128,128))\n",
        "    input_mask = tf.image.resize(datapoint['segmentation_mask'], (128,128))\n",
        "    input_image, input_mask = normalize(input_image, input_mask)\n",
        "    return tf.convert_to_tensor(input_image), tf.convert_to_tensor(input_mask)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8HzeiDNFp5H"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "BUFFER_SIZE = 1500\n",
        "dataset = {\"train\": train_dataset, \"val\": val_dataset}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34PHGsxqF2fT"
      },
      "outputs": [],
      "source": [
        "# Preparing the Train dataset by applying dataset transformations\n",
        "dataset['train'] = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "dataset['train'] = dataset['train'].shuffle(buffer_size=BUFFER_SIZE, seed=SEED)\n",
        "dataset['train'] = dataset['train'].repeat()\n",
        "dataset['train'] = dataset['train'].batch(BATCH_SIZE)\n",
        "dataset['train'] = dataset['train'].prefetch(buffer_size=AUTOTUNE)\n",
        "print(dataset['train'])\n",
        "\n",
        "# Preparing the Validation Dataset\n",
        "dataset['val'] = dataset['val'].map(load_image_test)\n",
        "dataset['val'] = dataset['val'].repeat()\n",
        "dataset['val'] = dataset['val'].batch(BATCH_SIZE)\n",
        "dataset['val'] = dataset['val'].prefetch(buffer_size=AUTOTUNE)\n",
        "print(dataset['val'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiAuwA4LGCnb"
      },
      "source": [
        "# Visualizing after processing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MByetfcYlnbY"
      },
      "outputs": [],
      "source": [
        "def display_sample(display_list):\n",
        "    \"\"\"\n",
        "    Show side-by-side an input image,\n",
        "    the ground truth and the prediction.\n",
        "\n",
        "  SHOWS INPUT IMAGE, TRUE MASK, PREDICTED IMAGE - these are in form of list sent in predict function.\n",
        "\n",
        "    The given display list is in form of array\n",
        "    3rd line in for loop converts the array into image\n",
        "    plt.axis is used to show grid\n",
        "    \"\"\"\n",
        "\n",
        "    plt.figure(figsize=(15,15))\n",
        "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
        "    for i in range(len(display_list)):\n",
        "        plt.subplot(1, len(display_list), i+1)\n",
        "        plt.title(title[i])\n",
        "        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQ9TtGdlGMCb"
      },
      "outputs": [],
      "source": [
        "# Getting a sample image for visualizing\n",
        "for image, mask in dataset['train'].take(1):\n",
        "    sample_image, sample_mask = image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hB3jmFBSJaN6"
      },
      "outputs": [],
      "source": [
        "# Displaying a sample Input Image and its corresponding mask\n",
        "display_sample([sample_image[0], sample_mask[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNY3v1lCZmdh"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqWRwEArWoek"
      },
      "source": [
        "# Model Building\n",
        "## U-net ArchitectureModel Building\n",
        "## U-net Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amBUMyBx_DMD"
      },
      "outputs": [],
      "source": [
        "# This class is for building the Encoder Block\n",
        "# Each encoder block (except the first block) consists of 3 layers ->\n",
        "# Max_Pooling -> Convolution_1 -> Convolution_2\n",
        "class EncoderBlock(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, filter_size, block_number, initializer='he_normal'):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        # this variable gives the information about the number of the encoder block\n",
        "        # this is useful as the first encoder block has a different structure with no max pool layer\n",
        "        self.block_number = block_number\n",
        "        # Each encoder block will have different number of filters\n",
        "        self.filter_size = filter_size\n",
        "        # defining the max pool layer with size 2\n",
        "        self.max_pool_layer = MaxPooling2D(pool_size=(2,2))\n",
        "        # defining the convolution layers\n",
        "        self.convolution_layer_1 = Conv2D(self.filter_size, 3, activation = 'relu', padding='same', kernel_initializer=initializer)\n",
        "        self.convolution_layer_2 = Conv2D(self.filter_size, 3, activation = 'relu', padding='same', kernel_initializer=initializer)\n",
        "\n",
        "    def call(self,inputs):\n",
        "        # as the first block doesn't have max pool layer we have to define its structure only if the block number is 1\n",
        "        if(self.block_number==1):\n",
        "            # the inputs will be passed to the first convolution layer\n",
        "            x = self.convolution_layer_1(inputs)\n",
        "            # The output of first convolution layer will be passed to second convolution layer\n",
        "            x = self.convolution_layer_2(x)\n",
        "            return x\n",
        "        # After the first block, for all the encoder blocks the output will be passed to a max_pool layer first\n",
        "        x = self.max_pool_layer(inputs)\n",
        "        # The Max pool out will be passed to first convolution layer\n",
        "        x = self.convolution_layer_1(x)\n",
        "        # The output of first convolution layer will be passed to the second convolution layer\n",
        "        x = self.convolution_layer_2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1Yo-Nc4Y99C"
      },
      "outputs": [],
      "source": [
        "#pip install keras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nmaSh0RYDhx"
      },
      "outputs": [],
      "source": [
        "\n",
        "# This class is for building the Decoder Block\n",
        "# Each encoder block (except the last block) consists of 4 layers ->\n",
        "# UpSampling -> Convolution_1 -> Concatenate -> Convolution_2 -> Convolution_3\n",
        "class DecoderBlock(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, filter_size, block_number, initializer='he_normal'):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        # Each decoder block has different number of filters\n",
        "        self.filter_size = filter_size\n",
        "        # this variable gives the information about the number of the decoder block\n",
        "        # this is useful as the layer decoder block has a different structure with an extra convolution layer\n",
        "        self.block_number = block_number\n",
        "        # defining the UpSampling layer\n",
        "        self.upsampling_layer = UpSampling2D(size=(2,2))\n",
        "        # defining the convolution layers\n",
        "        self.convolution_layer_1 = Conv2D(self.filter_size, 2, activation = 'relu', padding='same', kernel_initializer=initializer)\n",
        "        self.convolution_layer_2 = Conv2D(self.filter_size, 3, activation = 'relu', padding='same', kernel_initializer=initializer)\n",
        "        self.convolution_layer_3 = Conv2D(self.filter_size, 3, activation = 'relu', padding='same', kernel_initializer=initializer)\n",
        "        # defining the convolution layer for last decoder block\n",
        "        self.convolution_layer_4 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer)\n",
        "\n",
        "    def call(self,inputs,encoder_layer):\n",
        "        # The input is first passed to the UpSampling layer and then passed to the convolution layer\n",
        "        x = self.convolution_layer_1(self.upsampling_layer(inputs))\n",
        "        # The output of the convolution layer is concatenated with corresponding encoder layer output\n",
        "        merged = tf.concat([encoder_layer,x], axis=3)\n",
        "        # The merged output is passed to second convolution layer\n",
        "        x = self.convolution_layer_2(merged)\n",
        "        # The output of the second convolution layer is passed to the third convolution layer\n",
        "        x = self.convolution_layer_3(x)\n",
        "        # For the last decoder block, one more convolution layer is added with filter_size = 2\n",
        "        if(self.block_number==4):\n",
        "            x = self.convolution_layer_4(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvdtkIMZYRpa"
      },
      "outputs": [],
      "source": [
        "# This class defines the entire Segmentation Model\n",
        "# There are 5 Encoder Blocks and 4 Decoder blocks with convolution layer as the last layer\n",
        "# Encoder_64 -> Encoder_128 -> Encoder_256 -> Encoder_512 -> Encoder_1024 ->\n",
        "# Decoder_512 -> Decoder_256 -> Deocoder_128 -> Decoder_64 ->\n",
        "# Convolution_layer (with filter size as number_of_output_channels)\n",
        "class SegmentationModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, output_channels, initializer = 'he_normal', dynamic=True):\n",
        "        super(SegmentationModel, self).__init__()\n",
        "        # getting the number of output channels\n",
        "        self.output_channels = output_channels\n",
        "        # defining the encoder blocks with filter sizes 64,128,256,512,1024\n",
        "        self.encoder_block_1 = EncoderBlock(64,1)\n",
        "        self.encoder_block_2 = EncoderBlock(128,2)\n",
        "        self.encoder_block_3 = EncoderBlock(256,3)\n",
        "        self.encoder_block_4 = EncoderBlock(512,4)\n",
        "        self.encoder_block_5 = EncoderBlock(1024,5)\n",
        "        # defining the decoder blocks with filter sizes 512,256,128,64\n",
        "        self.decoder_block_1 = DecoderBlock(512,1)\n",
        "        self.decoder_block_2 = DecoderBlock(256,2)\n",
        "        self.decoder_block_3 = DecoderBlock(128,3)\n",
        "        self.decoder_block_4 = DecoderBlock(64,4)\n",
        "        # defining the last convolution layer with filter_size as number_of_output_channels\n",
        "        self.output_layer = Conv2D(self.output_channels, 1, activation='softmax')\n",
        "\n",
        "    def call(self,inputs):\n",
        "        # The input is passed to the first encoder block\n",
        "        conv_enc_1 = self.encoder_block_1(inputs)\n",
        "        # Output of each encoder block is passed to the next encoder block\n",
        "        conv_enc_2 = self.encoder_block_2(conv_enc_1)\n",
        "        conv_enc_3 = self.encoder_block_3(conv_enc_2)\n",
        "        conv_enc_4 = self.encoder_block_4(conv_enc_3)\n",
        "        conv_enc_5 = self.encoder_block_5(conv_enc_4)\n",
        "        # Output of the previous layer along with the outputs of encoder blocks\n",
        "        # in reverse orders are passed to decoder layers\n",
        "        conv_dec_1 = self.decoder_block_1(conv_enc_5,conv_enc_4)\n",
        "        conv_dec_2 = self.decoder_block_2(conv_dec_1,conv_enc_3)\n",
        "        conv_dec_3 = self.decoder_block_3(conv_dec_2,conv_enc_2)\n",
        "        conv_dec_4 = self.decoder_block_4(conv_dec_3,conv_enc_1)\n",
        "        # Output of the last Decoder layer is passed to the Convolution layer\n",
        "        output = self.output_layer(conv_dec_4)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHZevyl4aOV3"
      },
      "outputs": [],
      "source": [
        "# Image size that we are going to use\n",
        "(HEIGHT,WIDTH) = (128,256)\n",
        "# Our images are RGB (3 channels)\n",
        "N_CHANNELS = 3\n",
        "# Scene Parsing has 7 classes (0-6) + `not labeled`\n",
        "N_CLASSES = 8\n",
        "#from tensorflow.keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.layers import UpSampling2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbnYkPIuYZ1p"
      },
      "outputs": [],
      "source": [
        "# SegmentationModel object\n",
        "model = SegmentationModel(N_CLASSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBYONNEMYdOO"
      },
      "outputs": [],
      "source": [
        "# Defining a loss object and an optimizer\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, net=model)\n",
        "manager = tf.train.CheckpointManager(ckpt, 'tf_ckpts/', max_to_keep=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqKsJAMEYg86"
      },
      "outputs": [],
      "source": [
        "# Define the metrics\n",
        "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')\n",
        "test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBbmcTAOYkh8"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(model, optimizer, x_train, y_train):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(x_train, training=True)\n",
        "        loss = loss_object(y_train, predictions)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    train_loss(loss)\n",
        "    train_accuracy(y_train, predictions)\n",
        "\n",
        "def train_and_checkpoint(model, manager, dataset, epoch):\n",
        "    ckpt.restore(manager.latest_checkpoint)\n",
        "    if manager.latest_checkpoint:\n",
        "        print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
        "    else:\n",
        "        print(\"Initializing from scratch.\")\n",
        "    for (x_train, y_train) in dataset['train'].take(math.ceil(1403/32)):\n",
        "        tf.convert_to_tensor(x_train)\n",
        "        tf.convert_to_tensor(y_train)\n",
        "        train_step(model, optimizer, x_train, y_train)\n",
        "    ckpt.step.assign_add(1)\n",
        "    save_path = manager.save()\n",
        "    print(\"Saved checkpoint for epoch {}: {}\".format(epoch, save_path))\n",
        "\n",
        "@tf.function\n",
        "def test_step(model, x_test, y_test):\n",
        "    predictions = model(x_test)\n",
        "    loss = loss_object(y_test, predictions)\n",
        "    test_loss(loss)\n",
        "    test_accuracy(y_test, predictions)\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8AlK-44Yo6d"
      },
      "outputs": [],
      "source": [
        "# Summary writers for Tensorboard visualization\n",
        "train_log_dir = 'logs/gradient_tape/train'\n",
        "test_log_dir = 'logs/gradient_tape/test'\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eg1PatY_YsDR"
      },
      "outputs": [],
      "source": [
        "# This variable will help to save the best model if its performance increases after an epoch\n",
        "highest_accuracy = 0\n",
        "\n",
        "for epoch in range(150):\n",
        "\n",
        "    print(\"Epoch \",epoch+1)\n",
        "\n",
        "    # Getting the current time before starting the training\n",
        "    # This will help to keep track of how much time an epoch took\n",
        "    start = time.time()\n",
        "\n",
        "    train_and_checkpoint(model, manager, dataset, epoch+1)                              #error line\n",
        "\n",
        "    # Saving the train loss and train accuracy metric for TensorBoard visualization\n",
        "    with train_summary_writer.as_default():\n",
        "        tf.summary.scalar('loss', train_loss.result(), step=ckpt.step.numpy())\n",
        "        tf.summary.scalar('accuracy', train_accuracy.result(), step=ckpt.step.numpy())\n",
        "\n",
        "    # Validation phase\n",
        "    for (x_test, y_test) in dataset['val'].take(math.ceil(204/32)):\n",
        "      a = 0\n",
        "      pred = test_step(model, x_test, y_test)                                            #error line\n",
        "\n",
        "    # Saving the validation loss and validation accuracy metric for Tensorboard visualization\n",
        "    with test_summary_writer.as_default():\n",
        "        tf.summary.scalar('loss', test_loss.result(), step=ckpt.step.numpy())\n",
        "        tf.summary.scalar('accuracy', test_accuracy.result(), step=ckpt.step.numpy())\n",
        "\n",
        "    # Calculating the time it took for the entire epoch to run\n",
        "    print(\"Time taken \",time.time()-start)\n",
        "\n",
        "    # Printing the metrics for the epoch\n",
        "    template = 'Epoch {}, Loss: {:.3f}, Accuracy: {:.3f}, Val Loss: {:.3f}, Val Accuracy: {:.3f}'\n",
        "    print (template.format(epoch+1,\n",
        "                            train_loss.result(),\n",
        "                            train_accuracy.result()*100,\n",
        "                            test_loss.result(),\n",
        "                            test_accuracy.result()*100))\n",
        "\n",
        "    # If accuracy has increased in this epoch, updating the highest accuracy and saving the model\n",
        "    if(test_accuracy.result().numpy()*100>highest_accuracy):\n",
        "        print(\"Validation accuracy increased from {:.3f} to {:.3f}. Saving model weights.\".format(highest_accuracy,test_accuracy.result().numpy()*100))\n",
        "        highest_accuracy = test_accuracy.result().numpy()*100\n",
        "        model.save_weights('unet_weights-epoch-{}.hdf5'.format(epoch+1))\n",
        "\n",
        "    print('_'*80)\n",
        "\n",
        "    # Reset metrics after every epoch\n",
        "    train_loss.reset_states()\n",
        "    test_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_accuracy.reset_states()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Z_ADegaBUXd"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjXRQHIOBYjY"
      },
      "outputs": [],
      "source": [
        "\n",
        "%tensorboard --logdir logs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07WO23kU6ATv"
      },
      "outputs": [],
      "source": [
        "# Loading the weights of the best model\n",
        "model.load_weights('/content/drive/MyDrive/Check Points/tf_ckpts/unet_weights-epoch-49.hdf5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcc-M08w6uyU"
      },
      "outputs": [],
      "source": [
        "def predict(model,image_path):\n",
        "    \"\"\"\n",
        "    This function will take the model which is going to be used to predict the image and the image path of\n",
        "    the input image as inputs and predict the mask\n",
        "    It returns the true mask and predicted mask\n",
        "    \"\"\"\n",
        "    # Getting the datapoint\n",
        "    # This function will load the image and its annotation (mask) and return a dictionary.\n",
        "    datapoint = parse_image(image_path)\n",
        "    # Normalizing the resizing the datapoint\n",
        "    input_image,image_mask = load_image_test(datapoint)\n",
        "    # As the model takes input with 4 dimensions (batch_size, rows, columns, channels),\n",
        "    # and the shape of the input image is (rows, columns, channels)\n",
        "    # we will expand the first dimension so we will get the shape as  (1, rows, columns, channels)\n",
        "    img = tf.expand_dims(input_image, 0)\n",
        "    # Predicting the image by passing it to the model\n",
        "    prediction = model(img)\n",
        "    # The model will predict 8 outputs for each pixel\n",
        "    # We have to get the maximum value out of it\n",
        "    prediction = tf.argmax(prediction, axis=-1)\n",
        "    prediction = tf.squeeze(prediction, axis = 0)\n",
        "    pred_mask = tf.expand_dims(prediction, axis=-1)\n",
        "    # Displaying the input image, true mask, predicted mask\n",
        "    display_sample([input_image, image_mask, pred_mask])\n",
        "    return image_mask, pred_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9DCdmPl8m4A"
      },
      "outputs": [],
      "source": [
        "# Calling the predict function\n",
        "true_mask, pred_mask = predict(model,'/content/drive/MyDrive/segmentation_kaggle_dataset_ceci_project_1/new_idd_dataset_kaggle/idd20k_lite/leftImg8bit/val/17/425212_image.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OO-DMx5R9GlU"
      },
      "outputs": [],
      "source": [
        "# Reference -> https://github.com/saisandeepNSS/IDD_SemanticSegmentation\n",
        "\n",
        "def IoU(y_i,y_pred):\n",
        "    # This function calculates the mean Intersection over Union\n",
        "    # Mean IoU = TP/(FN + TP + FP)\n",
        "    # This list will save the IoU of all the classes\n",
        "    IoUs = []\n",
        "    # Defining the number of classes which the model has predicted\n",
        "    n_classes = 8\n",
        "    for c in range(n_classes):\n",
        "        # Calculating the True Positives\n",
        "        TP = np.sum((y_i == c)&(y_pred==c))\n",
        "        # Calculating the False Positives\n",
        "        FP = np.sum((y_i != c)&(y_pred==c))\n",
        "        # Calculating the False Negatives\n",
        "        FN = np.sum((y_i == c)&(y_pred!= c))\n",
        "        # Calculating the IoU for the particular class\n",
        "        IoU = TP/float(TP + FP + FN)\n",
        "        # Printing the outputs\n",
        "        print(\"class {:02.0f}: #TP={:6.0f}, #FP={:6.0f}, #FN={:5.0f}, IoU={:4.3f}\".format(c,TP,FP,FN,IoU))\n",
        "        # Appending the IoU to the list as it mean needs to be calculated later\n",
        "        if(math.isnan(IoU)):\n",
        "            IoUs.append(0)\n",
        "            continue\n",
        "        IoUs.append(IoU)\n",
        "    # Calculating the mean\n",
        "    mIoU = np.mean(IoUs)\n",
        "    print(\"_________________\")\n",
        "    print(\"Mean IoU: {:4.3f}\".format(mIoU))\n",
        "    return mIoU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpl1Pg9o9Krm"
      },
      "outputs": [],
      "source": [
        "IoU(true_mask, pred_mask)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}